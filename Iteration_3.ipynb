{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b1ef9cc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 8523 entries, 0 to 8522\n",
      "Data columns (total 12 columns):\n",
      " #   Column                     Non-Null Count  Dtype  \n",
      "---  ------                     --------------  -----  \n",
      " 0   Item_Identifier            8523 non-null   object \n",
      " 1   Item_Weight                7060 non-null   float64\n",
      " 2   Item_Fat_Content           8523 non-null   object \n",
      " 3   Item_Visibility            8523 non-null   float64\n",
      " 4   Item_Type                  8523 non-null   object \n",
      " 5   Item_MRP                   8523 non-null   float64\n",
      " 6   Outlet_Identifier          8523 non-null   object \n",
      " 7   Outlet_Establishment_Year  8523 non-null   int64  \n",
      " 8   Outlet_Size                6113 non-null   object \n",
      " 9   Outlet_Location_Type       8523 non-null   object \n",
      " 10  Outlet_Type                8523 non-null   object \n",
      " 11  Item_Outlet_Sales          8523 non-null   float64\n",
      "dtypes: float64(4), int64(1), object(7)\n",
      "memory usage: 799.2+ KB\n",
      "None\n",
      "       Item_Weight  Item_Visibility     Item_MRP  Outlet_Establishment_Year  \\\n",
      "count  7060.000000      8523.000000  8523.000000                8523.000000   \n",
      "mean     12.857645         0.066132   140.992782                1997.831867   \n",
      "std       4.643456         0.051598    62.275067                   8.371760   \n",
      "min       4.555000         0.000000    31.290000                1985.000000   \n",
      "25%       8.773750         0.026989    93.826500                1987.000000   \n",
      "50%      12.600000         0.053931   143.012800                1999.000000   \n",
      "75%      16.850000         0.094585   185.643700                2004.000000   \n",
      "max      21.350000         0.328391   266.888400                2009.000000   \n",
      "\n",
      "       Item_Outlet_Sales  \n",
      "count        8523.000000  \n",
      "mean         2181.288914  \n",
      "std          1706.499616  \n",
      "min            33.290000  \n",
      "25%           834.247400  \n",
      "50%          1794.331000  \n",
      "75%          3101.296400  \n",
      "max         13086.964800  \n",
      "RF Validation RMSE: 1081.1206721180406\n",
      "Submission file created successfully!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "\n",
    "# Load the datasets\n",
    "train_data = pd.read_csv('train.csv')\n",
    "test_data = pd.read_csv('test.csv')\n",
    "\n",
    "# Data Exploration\n",
    "print(train_data.info())\n",
    "print(train_data.describe())\n",
    "\n",
    "# Handle missing values\n",
    "train_data['Item_Weight'].fillna(train_data['Item_Weight'].mean(), inplace=True)\n",
    "test_data['Item_Weight'].fillna(test_data['Item_Weight'].mean(), inplace=True)\n",
    "\n",
    "train_data['Outlet_Size'].fillna(train_data['Outlet_Size'].mode()[0], inplace=True)\n",
    "test_data['Outlet_Size'].fillna(test_data['Outlet_Size'].mode()[0], inplace=True)\n",
    "\n",
    "# Encode categorical variables\n",
    "label_encoders = {}\n",
    "categorical_columns = ['Item_Fat_Content', 'Item_Type', 'Outlet_Identifier', 'Outlet_Size', 'Outlet_Location_Type', 'Outlet_Type']\n",
    "\n",
    "for col in categorical_columns:\n",
    "    le = LabelEncoder()\n",
    "    train_data[col] = le.fit_transform(train_data[col])\n",
    "    test_data[col] = le.transform(test_data[col])\n",
    "    label_encoders[col] = le\n",
    "\n",
    "# Feature Engineering\n",
    "train_data['Outlet_Age'] = 2013 - train_data['Outlet_Establishment_Year']\n",
    "test_data['Outlet_Age'] = 2013 - test_data['Outlet_Establishment_Year']\n",
    "\n",
    "# Drop unnecessary columns\n",
    "train_data.drop(['Item_Identifier', 'Outlet_Establishment_Year'], axis=1, inplace=True)\n",
    "test_data.drop(['Item_Identifier', 'Outlet_Establishment_Year'], axis=1, inplace=True)\n",
    "\n",
    "# Split the data into features and target\n",
    "X = train_data.drop('Item_Outlet_Sales', axis=1)\n",
    "y = train_data['Item_Outlet_Sales']\n",
    "\n",
    "# Split the data into training and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Standardize the features\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_val = scaler.transform(X_val)\n",
    "test_data_scaled = scaler.transform(test_data)\n",
    "\n",
    "# Model Selection and Training\n",
    "model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Model Validation\n",
    "y_pred = model.predict(X_val)\n",
    "rmse = np.sqrt(mean_squared_error(y_val, y_pred))\n",
    "print(f'RF Validation RMSE: {rmse}')\n",
    "\n",
    "# Prediction on Test Data\n",
    "test_predictions = model.predict(test_data_scaled)\n",
    "\n",
    "# Prepare Submission File\n",
    "submission = pd.DataFrame({\n",
    "    'Item_Identifier': pd.read_csv('test.csv')['Item_Identifier'],\n",
    "    'Outlet_Identifier': pd.read_csv('test.csv')['Outlet_Identifier'],\n",
    "    'Item_Outlet_Sales': test_predictions\n",
    "})\n",
    "\n",
    "submission.to_csv('submission3.csv', index=False)\n",
    "print(\"Submission file created successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4de145ec",
   "metadata": {},
   "source": [
    "### Iteration 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a5445221",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation RMSE: 0.5337369411719414\n",
      "Submission file created successfully!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "# Load the datasets\n",
    "train_data = pd.read_csv('train.csv')\n",
    "test_data = pd.read_csv('test.csv')\n",
    "\n",
    "# Feature Engineering\n",
    "# Extract Item_Category from Item_Identifier\n",
    "train_data['Item_Category'] = train_data['Item_Identifier'].apply(lambda x: x[:2])\n",
    "test_data['Item_Category'] = test_data['Item_Identifier'].apply(lambda x: x[:2])\n",
    "\n",
    "# Bin Item_MRP into price tiers\n",
    "train_data['Price_Tier'] = pd.cut(train_data['Item_MRP'], bins=[0, 50, 100, 150, 200, 250], labels=[1, 2, 3, 4, 5])\n",
    "test_data['Price_Tier'] = pd.cut(test_data['Item_MRP'], bins=[0, 50, 100, 150, 200, 250], labels=[1, 2, 3, 4, 5])\n",
    "\n",
    "# Bin Item_Visibility into categories\n",
    "train_data['Visibility_Bins'] = pd.cut(train_data['Item_Visibility'], bins=[-1, 0.1, 0.2, 0.3, 1], labels=[1, 2, 3, 4])\n",
    "test_data['Visibility_Bins'] = pd.cut(test_data['Item_Visibility'], bins=[-1, 0.1, 0.2, 0.3, 1], labels=[1, 2, 3, 4])\n",
    "\n",
    "# Handle missing values\n",
    "train_data['Item_Weight'].fillna(train_data['Item_Weight'].mean(), inplace=True)\n",
    "test_data['Item_Weight'].fillna(test_data['Item_Weight'].mean(), inplace=True)\n",
    "\n",
    "train_data['Outlet_Age'] = 2013 - train_data['Outlet_Establishment_Year']\n",
    "test_data['Outlet_Age'] = 2013 - test_data['Outlet_Establishment_Year']\n",
    "\n",
    "train_data['Outlet_Size'].fillna(train_data['Outlet_Size'].mode()[0], inplace=True)\n",
    "test_data['Outlet_Size'].fillna(test_data['Outlet_Size'].mode()[0], inplace=True)\n",
    "\n",
    "# Drop unnecessary columns\n",
    "train_data.drop(['Item_Identifier', 'Outlet_Establishment_Year'], axis=1, inplace=True)\n",
    "test_data.drop(['Item_Identifier', 'Outlet_Establishment_Year'], axis=1, inplace=True)\n",
    "\n",
    "# Log transformation of the target variable\n",
    "y = np.log1p(train_data['Item_Outlet_Sales'])  # log(1 + x) to handle zero sales\n",
    "X = train_data.drop('Item_Outlet_Sales', axis=1)\n",
    "\n",
    "# Define categorical and numerical columns\n",
    "categorical_cols = ['Item_Fat_Content', 'Item_Type', 'Outlet_Identifier', 'Outlet_Size', 'Outlet_Location_Type', 'Outlet_Type', 'Item_Category', 'Price_Tier', 'Visibility_Bins']\n",
    "numerical_cols = ['Item_Weight', 'Item_Visibility', 'Item_MRP', 'Outlet_Age']\n",
    "\n",
    "# Preprocessing pipeline\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', StandardScaler(), numerical_cols),\n",
    "        ('cat', OneHotEncoder(handle_unknown='ignore'), categorical_cols)\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Model Pipeline\n",
    "model = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('regressor', XGBRegressor(random_state=42, n_estimators=200, max_depth=6, learning_rate=0.1))\n",
    "])\n",
    "\n",
    "# Split the data into training and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Validate the model\n",
    "y_pred = model.predict(X_val)\n",
    "rmse = np.sqrt(mean_squared_error(y_val, y_pred))\n",
    "print(f'Validation RMSE: {rmse}')\n",
    "\n",
    "# Predict on the test data\n",
    "test_predictions = model.predict(test_data)\n",
    "\n",
    "# Prepare Submission File\n",
    "submission = pd.DataFrame({\n",
    "    'Item_Identifier': pd.read_csv('test.csv')['Item_Identifier'],\n",
    "    'Outlet_Identifier': pd.read_csv('test.csv')['Outlet_Identifier'],\n",
    "    'Item_Outlet_Sales': test_predictions\n",
    "})\n",
    "\n",
    "submission.to_csv('submission4.csv', index=False)\n",
    "print(\"Submission file created successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b68c8c7f",
   "metadata": {},
   "source": [
    "### Iteration 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fc734795",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBoost Validation RMSE: 0.5337369411719414\n",
      "LightGBM Validation RMSE: 0.5298734232399103\n",
      "\n",
      "LightGBM\n",
      "Submission file created successfully!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, KFold, GridSearchCV\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from xgboost import XGBRegressor\n",
    "import lightgbm as lgb\n",
    "\n",
    "# Load the datasets\n",
    "train_data = pd.read_csv('train.csv')\n",
    "test_data = pd.read_csv('test.csv')\n",
    "\n",
    "# Feature Engineering\n",
    "# Extract Item_Category from Item_Identifier\n",
    "train_data['Item_Category'] = train_data['Item_Identifier'].apply(lambda x: x[:2])\n",
    "test_data['Item_Category'] = test_data['Item_Identifier'].apply(lambda x: x[:2])\n",
    "\n",
    "# Bin Item_MRP into price tiers\n",
    "train_data['Price_Tier'] = pd.cut(train_data['Item_MRP'], bins=[0, 50, 100, 150, 200, 250], labels=[1, 2, 3, 4, 5])\n",
    "test_data['Price_Tier'] = pd.cut(test_data['Item_MRP'], bins=[0, 50, 100, 150, 200, 250], labels=[1, 2, 3, 4, 5])\n",
    "\n",
    "# Bin Item_Visibility into categories\n",
    "train_data['Visibility_Bins'] = pd.cut(train_data['Item_Visibility'], bins=[-1, 0.1, 0.2, 0.3, 1], labels=[1, 2, 3, 4])\n",
    "test_data['Visibility_Bins'] = pd.cut(test_data['Item_Visibility'], bins=[-1, 0.1, 0.2, 0.3, 1], labels=[1, 2, 3, 4])\n",
    "\n",
    "# Handle missing values\n",
    "train_data['Item_Weight'].fillna(train_data['Item_Weight'].mean(), inplace=True)\n",
    "test_data['Item_Weight'].fillna(test_data['Item_Weight'].mean(), inplace=True)\n",
    "\n",
    "train_data['Outlet_Age'] = 2013 - train_data['Outlet_Establishment_Year']\n",
    "test_data['Outlet_Age'] = 2013 - test_data['Outlet_Establishment_Year']\n",
    "\n",
    "train_data['Outlet_Size'].fillna(train_data['Outlet_Size'].mode()[0], inplace=True)\n",
    "test_data['Outlet_Size'].fillna(test_data['Outlet_Size'].mode()[0], inplace=True)\n",
    "\n",
    "# Drop unnecessary columns\n",
    "train_data.drop(['Item_Identifier', 'Outlet_Establishment_Year'], axis=1, inplace=True)\n",
    "test_data.drop(['Item_Identifier', 'Outlet_Establishment_Year'], axis=1, inplace=True)\n",
    "\n",
    "# Log transformation of the target variable\n",
    "y = np.log1p(train_data['Item_Outlet_Sales'])  # log(1 + x) to handle zero sales\n",
    "X = train_data.drop('Item_Outlet_Sales', axis=1)\n",
    "\n",
    "# Define categorical and numerical columns\n",
    "categorical_cols = ['Item_Fat_Content', 'Item_Type', 'Outlet_Identifier', 'Outlet_Size', 'Outlet_Location_Type', 'Outlet_Type', 'Item_Category', 'Price_Tier', 'Visibility_Bins']\n",
    "numerical_cols = ['Item_Weight', 'Item_Visibility', 'Item_MRP', 'Outlet_Age']\n",
    "\n",
    "# Preprocessing pipeline\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', StandardScaler(), numerical_cols),\n",
    "        ('cat', OneHotEncoder(handle_unknown='ignore'), categorical_cols)\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Model Pipeline\n",
    "xgb_model = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('regressor', XGBRegressor(random_state=42, n_estimators=200, max_depth=6, learning_rate=0.1))\n",
    "])\n",
    "\n",
    "lgb_model = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('regressor', lgb.LGBMRegressor(random_state=42, n_estimators=200, max_depth=6, learning_rate=0.1))\n",
    "])\n",
    "\n",
    "# Split the data into training and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train and evaluate XGBoost model\n",
    "xgb_model.fit(X_train, y_train)\n",
    "y_pred_xgb = xgb_model.predict(X_val)\n",
    "rmse_xgb = np.sqrt(mean_squared_error(y_val, y_pred_xgb))\n",
    "print(f'XGBoost Validation RMSE: {rmse_xgb}')\n",
    "\n",
    "# Train and evaluate LightGBM model\n",
    "lgb_model.fit(X_train, y_train)\n",
    "y_pred_lgb = lgb_model.predict(X_val)\n",
    "rmse_lgb = np.sqrt(mean_squared_error(y_val, y_pred_lgb))\n",
    "print(f'LightGBM Validation RMSE: {rmse_lgb}')\n",
    "\n",
    "if(rmse_xgb<rmse_lgb):\n",
    "    model = xgb_model\n",
    "    print('\\nXGBoost')\n",
    "else:\n",
    "    model = lgb_model\n",
    "    print('\\nLightGBM')\n",
    "\n",
    "# Predict on the test data\n",
    "test_predictions = model.predict(test_data)\n",
    "\n",
    "# Prepare Submission File\n",
    "submission = pd.DataFrame({\n",
    "    'Item_Identifier': pd.read_csv('test.csv')['Item_Identifier'],\n",
    "    'Outlet_Identifier': pd.read_csv('test.csv')['Outlet_Identifier'],\n",
    "    'Item_Outlet_Sales': test_predictions\n",
    "})\n",
    "\n",
    "submission.to_csv('submission5.csv', index=False)\n",
    "print(\"Submission file created successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c9e1baa",
   "metadata": {},
   "source": [
    "### Iteration 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "776159fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Hyperparameters: {'regressor__max_depth': 10, 'regressor__min_samples_leaf': 1, 'regressor__min_samples_split': 10, 'regressor__n_estimators': 200}\n",
      "RF Training RMSE: 0.45296043965735666\n",
      "Submission file created successfully!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, KFold\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Load the datasets\n",
    "train_data = pd.read_csv('train.csv')\n",
    "test_data = pd.read_csv('test.csv')\n",
    "\n",
    "# Feature Engineering\n",
    "# Extract Item_Category from Item_Identifier\n",
    "train_data['Item_Category'] = train_data['Item_Identifier'].apply(lambda x: x[:2])\n",
    "test_data['Item_Category'] = test_data['Item_Identifier'].apply(lambda x: x[:2])\n",
    "\n",
    "# Bin Item_MRP into price tiers\n",
    "train_data['Price_Tier'] = pd.cut(train_data['Item_MRP'], bins=[0, 50, 100, 150, 200, 250], labels=[1, 2, 3, 4, 5])\n",
    "test_data['Price_Tier'] = pd.cut(test_data['Item_MRP'], bins=[0, 50, 100, 150, 200, 250], labels=[1, 2, 3, 4, 5])\n",
    "\n",
    "# Bin Item_Visibility into categories\n",
    "train_data['Visibility_Bins'] = pd.cut(train_data['Item_Visibility'], bins=[-1, 0.1, 0.2, 0.3, 1], labels=[1, 2, 3, 4])\n",
    "test_data['Visibility_Bins'] = pd.cut(test_data['Item_Visibility'], bins=[-1, 0.1, 0.2, 0.3, 1], labels=[1, 2, 3, 4])\n",
    "\n",
    "train_data['Outlet_Age'] = 2013 - train_data['Outlet_Establishment_Year']\n",
    "test_data['Outlet_Age'] = 2013 - test_data['Outlet_Establishment_Year']\n",
    "\n",
    "# Handle missing values\n",
    "train_data['Item_Weight'].fillna(train_data['Item_Weight'].mean(), inplace=True)\n",
    "test_data['Item_Weight'].fillna(test_data['Item_Weight'].mean(), inplace=True)\n",
    "\n",
    "train_data['Outlet_Size'].fillna(train_data['Outlet_Size'].mode()[0], inplace=True)\n",
    "test_data['Outlet_Size'].fillna(test_data['Outlet_Size'].mode()[0], inplace=True)\n",
    "\n",
    "# Drop unnecessary columns\n",
    "train_data.drop(['Item_Identifier', 'Outlet_Establishment_Year'], axis=1, inplace=True)\n",
    "test_data.drop(['Item_Identifier', 'Outlet_Establishment_Year'], axis=1, inplace=True)\n",
    "\n",
    "# Log transformation of the target variable\n",
    "y = np.log1p(train_data['Item_Outlet_Sales'])  # log(1 + x) to handle zero sales\n",
    "X = train_data.drop('Item_Outlet_Sales', axis=1)\n",
    "\n",
    "# Define categorical and numerical columns\n",
    "categorical_cols = ['Item_Fat_Content', 'Item_Type', 'Outlet_Identifier', 'Outlet_Size', 'Outlet_Location_Type', 'Outlet_Type', 'Item_Category', 'Price_Tier', 'Visibility_Bins']\n",
    "numerical_cols = ['Item_Weight', 'Item_Visibility', 'Item_MRP', 'Outlet_Age']\n",
    "\n",
    "# Preprocessing pipeline\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', StandardScaler(), numerical_cols),\n",
    "        ('cat', OneHotEncoder(handle_unknown='ignore'), categorical_cols)\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Model Pipeline\n",
    "rf_model = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('regressor', RandomForestRegressor(random_state=42))\n",
    "])\n",
    "\n",
    "# Hyperparameter tuning using GridSearchCV\n",
    "param_grid = {\n",
    "    'regressor__n_estimators': [100, 200],\n",
    "    'regressor__max_depth': [None, 10, 20, 30],\n",
    "    'regressor__min_samples_split': [2, 5, 10],\n",
    "    'regressor__min_samples_leaf': [1, 2, 4],\n",
    "}\n",
    "\n",
    "# Use K-Fold Cross-Validation\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "grid_search = GridSearchCV(rf_model, param_grid, cv=kf, scoring='neg_root_mean_squared_error', n_jobs=-1)\n",
    "grid_search.fit(X, y)\n",
    "\n",
    "# Best model from grid search\n",
    "best_model = grid_search.best_estimator_\n",
    "print(f'Best Hyperparameters: {grid_search.best_params_}')\n",
    "\n",
    "# Validate the model using cross-validation\n",
    "y_pred = best_model.predict(X)\n",
    "rmse = np.sqrt(mean_squared_error(y, y_pred))\n",
    "print(f'RF Training RMSE: {rmse}')\n",
    "\n",
    "# Predict on the test data\n",
    "test_predictions = best_model.predict(test_data)\n",
    "\n",
    "# Convert predictions back from log scale\n",
    "test_predictions = np.expm1(test_predictions)\n",
    "\n",
    "# Prepare Submission File\n",
    "submission = pd.DataFrame({\n",
    "    'Item_Identifier': pd.read_csv('test.csv')['Item_Identifier'],\n",
    "    'Outlet_Identifier': pd.read_csv('test.csv')['Outlet_Identifier'],\n",
    "    'Item_Outlet_Sales': test_predictions\n",
    "})\n",
    "\n",
    "submission.to_csv('submission6.csv', index=False)\n",
    "print(\"Submission file created successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47b0df38",
   "metadata": {},
   "source": [
    "### Iteration 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8d826349",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training RMSE for Stacked Model: 0.46332093562046256\n",
      "Submission file created successfully!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, KFold, GridSearchCV\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, StackingRegressor\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "import lightgbm as lgb\n",
    "\n",
    "# Load the datasets\n",
    "train_data = pd.read_csv('train.csv')\n",
    "test_data = pd.read_csv('test.csv')\n",
    "\n",
    "# Feature Engineering\n",
    "# Extract Item_Category from Item_Identifier\n",
    "train_data['Item_Category'] = train_data['Item_Identifier'].apply(lambda x: x[:2])\n",
    "test_data['Item_Category'] = test_data['Item_Identifier'].apply(lambda x: x[:2])\n",
    "\n",
    "train_data['Outlet_Age'] = 2013 - train_data['Outlet_Establishment_Year']\n",
    "test_data['Outlet_Age'] = 2013 - test_data['Outlet_Establishment_Year'] \n",
    "\n",
    "# Bin Item_MRP into price tiers\n",
    "train_data['Price_Tier'] = pd.cut(train_data['Item_MRP'], bins=[0, 50, 100, 150, 200, 250], labels=[1, 2, 3, 4, 5])\n",
    "test_data['Price_Tier'] = pd.cut(test_data['Item_MRP'], bins=[0, 50, 100, 150, 200, 250], labels=[1, 2, 3, 4, 5])\n",
    "\n",
    "# Bin Item_Visibility into categories\n",
    "train_data['Visibility_Bins'] = pd.cut(train_data['Item_Visibility'], bins=[-1, 0.1, 0.2, 0.3, 1], labels=[1, 2, 3, 4])\n",
    "test_data['Visibility_Bins'] = pd.cut(test_data['Item_Visibility'], bins=[-1, 0.1, 0.2, 0.3, 1], labels=[1, 2, 3, 4])\n",
    "\n",
    "# Handle missing values\n",
    "train_data['Item_Weight'].fillna(train_data['Item_Weight'].mean(), inplace=True)\n",
    "test_data['Item_Weight'].fillna(test_data['Item_Weight'].mean(), inplace=True)\n",
    "\n",
    "train_data['Outlet_Size'].fillna(train_data['Outlet_Size'].mode()[0], inplace=True)\n",
    "test_data['Outlet_Size'].fillna(test_data['Outlet_Size'].mode()[0], inplace=True)\n",
    "\n",
    "# Drop unnecessary columns\n",
    "train_data.drop(['Item_Identifier', 'Outlet_Establishment_Year'], axis=1, inplace=True)\n",
    "test_data.drop(['Item_Identifier', 'Outlet_Establishment_Year'], axis=1, inplace=True)\n",
    "\n",
    "# Log transformation of the target variable\n",
    "y = np.log1p(train_data['Item_Outlet_Sales'])  # log(1 + x) to handle zero sales\n",
    "X = train_data.drop('Item_Outlet_Sales', axis=1)\n",
    "\n",
    "# Define categorical and numerical columns\n",
    "categorical_cols = ['Item_Fat_Content', 'Item_Type', 'Outlet_Identifier', 'Outlet_Size', 'Outlet_Location_Type', 'Outlet_Type', 'Item_Category', 'Price_Tier', 'Visibility_Bins']\n",
    "numerical_cols = ['Item_Weight', 'Item_Visibility', 'Item_MRP', 'Outlet_Age']\n",
    "\n",
    "# Preprocessing pipeline for base models\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', StandardScaler(), numerical_cols),\n",
    "        ('cat', OneHotEncoder(handle_unknown='ignore'), categorical_cols)\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Base models for stacking with preprocessing\n",
    "base_models = [\n",
    "    ('rf', Pipeline(steps=[('preprocessor', preprocessor), ('regressor', RandomForestRegressor(n_estimators=100, random_state=42))])),\n",
    "    ('lgb', Pipeline(steps=[('preprocessor', preprocessor), ('regressor', lgb.LGBMRegressor(n_estimators=100, random_state=42))]))\n",
    "]\n",
    "\n",
    "# Stacking Regressor\n",
    "stacked_model = StackingRegressor(estimators=base_models, final_estimator=GradientBoostingRegressor())\n",
    "\n",
    "# Use K-Fold Cross-Validation for stacking\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Fit the stacked model\n",
    "stacked_model.fit(X, y)\n",
    "\n",
    "# Validate the model using cross-validation\n",
    "y_pred_stacked = stacked_model.predict(X)\n",
    "rmse_stacked = np.sqrt(mean_squared_error(y, y_pred_stacked))\n",
    "print(f'Training RMSE for Stacked Model: {rmse_stacked}')\n",
    "\n",
    "# Predict on the test data\n",
    "test_predictions = stacked_model.predict(test_data)\n",
    "\n",
    "# Convert predictions back from log scale\n",
    "test_predictions = np.expm1(test_predictions)  # Convert back from log(1 + x)\n",
    "\n",
    "# Prepare Submission File\n",
    "submission = pd.DataFrame({\n",
    "    'Item_Identifier': pd.read_csv('test.csv')['Item_Identifier'],\n",
    "    'Outlet_Identifier': pd.read_csv('test.csv')['Outlet_Identifier'],\n",
    "    'Item_Outlet_Sales': test_predictions\n",
    "})\n",
    "\n",
    "submission.to_csv('submission7.csv', index=False)\n",
    "print(\"Submission file created successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e3d9035d",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "\n\nIMPORTANT: PLEASE READ THIS FOR ADVICE ON HOW TO SOLVE THIS ISSUE!\n\nImporting the numpy C-extensions failed. This error can happen for\nmany reasons, often due to issues with your setup or how NumPy was\ninstalled.\n\nWe have compiled some common reasons and troubleshooting tips at:\n\n    https://numpy.org/devdocs/user/troubleshooting-importerror.html\n\nPlease note and check the following:\n\n  * The Python version is: Python3.9 from \"C:\\Users\\INGRSHA\\Anaconda3\\python.exe\"\n  * The NumPy version is: \"1.21.6\"\n\nand make sure that they are the versions you expect.\nPlease carefully study the documentation linked above for further help.\n\nOriginal error was: DLL load failed while importing _multiarray_umath: The specified module could not be found.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\numpy\\_core\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 23\u001b[1;33m     \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmultiarray\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     24\u001b[0m \u001b[1;32mexcept\u001b[0m \u001b[0mImportError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mexc\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\numpy\\_core\\multiarray.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mfunctools\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0moverrides\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0m_multiarray_umath\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\numpy\\_core\\overrides.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_utils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_inspect\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mgetargspec\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m from numpy._core._multiarray_umath import (\n\u001b[0m\u001b[0;32m      9\u001b[0m     add_docstring,  _get_implementing_args, _ArrayFunctionDispatcher)\n",
      "\u001b[1;31mImportError\u001b[0m: DLL load failed while importing _multiarray_umath: The specified module could not be found.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\numpy\\_core\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     47\u001b[0m \"\"\" % (sys.version_info[0], sys.version_info[1], sys.executable,\n\u001b[0;32m     48\u001b[0m         __version__, exc)\n\u001b[1;32m---> 49\u001b[1;33m     \u001b[1;32mraise\u001b[0m \u001b[0mImportError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     50\u001b[0m \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     51\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0menvkey\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menv_added\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mImportError\u001b[0m: \n\nIMPORTANT: PLEASE READ THIS FOR ADVICE ON HOW TO SOLVE THIS ISSUE!\n\nImporting the numpy C-extensions failed. This error can happen for\nmany reasons, often due to issues with your setup or how NumPy was\ninstalled.\n\nWe have compiled some common reasons and troubleshooting tips at:\n\n    https://numpy.org/devdocs/user/troubleshooting-importerror.html\n\nPlease note and check the following:\n\n  * The Python version is: Python3.9 from \"C:\\Users\\INGRSHA\\Anaconda3\\python.exe\"\n  * The NumPy version is: \"1.21.6\"\n\nand make sure that they are the versions you expect.\nPlease carefully study the documentation linked above for further help.\n\nOriginal error was: DLL load failed while importing _multiarray_umath: The specified module could not be found.\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "\n\nIMPORTANT: PLEASE READ THIS FOR ADVICE ON HOW TO SOLVE THIS ISSUE!\n\nImporting the numpy C-extensions failed. This error can happen for\nmany reasons, often due to issues with your setup or how NumPy was\ninstalled.\n\nWe have compiled some common reasons and troubleshooting tips at:\n\n    https://numpy.org/devdocs/user/troubleshooting-importerror.html\n\nPlease note and check the following:\n\n  * The Python version is: Python3.9 from \"C:\\Users\\INGRSHA\\Anaconda3\\python.exe\"\n  * The NumPy version is: \"1.21.6\"\n\nand make sure that they are the versions you expect.\nPlease carefully study the documentation linked above for further help.\n\nOriginal error was: DLL load failed while importing _multiarray_umath: The specified module could not be found.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\numpy\\_core\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 23\u001b[1;33m     \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmultiarray\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     24\u001b[0m \u001b[1;32mexcept\u001b[0m \u001b[0mImportError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mexc\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\numpy\\_core\\multiarray.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mfunctools\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0moverrides\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0m_multiarray_umath\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\numpy\\_core\\overrides.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_utils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_inspect\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mgetargspec\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m from numpy._core._multiarray_umath import (\n\u001b[0m\u001b[0;32m      9\u001b[0m     add_docstring,  _get_implementing_args, _ArrayFunctionDispatcher)\n",
      "\u001b[1;31mImportError\u001b[0m: DLL load failed while importing _multiarray_umath: The specified module could not be found.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\numpy\\_core\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     47\u001b[0m \"\"\" % (sys.version_info[0], sys.version_info[1], sys.executable,\n\u001b[0;32m     48\u001b[0m         __version__, exc)\n\u001b[1;32m---> 49\u001b[1;33m     \u001b[1;32mraise\u001b[0m \u001b[0mImportError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     50\u001b[0m \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     51\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0menvkey\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menv_added\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mImportError\u001b[0m: \n\nIMPORTANT: PLEASE READ THIS FOR ADVICE ON HOW TO SOLVE THIS ISSUE!\n\nImporting the numpy C-extensions failed. This error can happen for\nmany reasons, often due to issues with your setup or how NumPy was\ninstalled.\n\nWe have compiled some common reasons and troubleshooting tips at:\n\n    https://numpy.org/devdocs/user/troubleshooting-importerror.html\n\nPlease note and check the following:\n\n  * The Python version is: Python3.9 from \"C:\\Users\\INGRSHA\\Anaconda3\\python.exe\"\n  * The NumPy version is: \"1.21.6\"\n\nand make sure that they are the versions you expect.\nPlease carefully study the documentation linked above for further help.\n\nOriginal error was: DLL load failed while importing _multiarray_umath: The specified module could not be found.\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "\n\nIMPORTANT: PLEASE READ THIS FOR ADVICE ON HOW TO SOLVE THIS ISSUE!\n\nImporting the numpy C-extensions failed. This error can happen for\nmany reasons, often due to issues with your setup or how NumPy was\ninstalled.\n\nWe have compiled some common reasons and troubleshooting tips at:\n\n    https://numpy.org/devdocs/user/troubleshooting-importerror.html\n\nPlease note and check the following:\n\n  * The Python version is: Python3.9 from \"C:\\Users\\INGRSHA\\Anaconda3\\python.exe\"\n  * The NumPy version is: \"1.21.6\"\n\nand make sure that they are the versions you expect.\nPlease carefully study the documentation linked above for further help.\n\nOriginal error was: DLL load failed while importing _multiarray_umath: The specified module could not be found.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\numpy\\_core\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 23\u001b[1;33m     \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmultiarray\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     24\u001b[0m \u001b[1;32mexcept\u001b[0m \u001b[0mImportError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mexc\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\numpy\\_core\\multiarray.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mfunctools\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0moverrides\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0m_multiarray_umath\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\numpy\\_core\\overrides.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_utils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_inspect\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mgetargspec\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m from numpy._core._multiarray_umath import (\n\u001b[0m\u001b[0;32m      9\u001b[0m     add_docstring,  _get_implementing_args, _ArrayFunctionDispatcher)\n",
      "\u001b[1;31mImportError\u001b[0m: DLL load failed while importing _multiarray_umath: The specified module could not be found.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\numpy\\_core\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     47\u001b[0m \"\"\" % (sys.version_info[0], sys.version_info[1], sys.executable,\n\u001b[0;32m     48\u001b[0m         __version__, exc)\n\u001b[1;32m---> 49\u001b[1;33m     \u001b[1;32mraise\u001b[0m \u001b[0mImportError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     50\u001b[0m \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     51\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0menvkey\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menv_added\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mImportError\u001b[0m: \n\nIMPORTANT: PLEASE READ THIS FOR ADVICE ON HOW TO SOLVE THIS ISSUE!\n\nImporting the numpy C-extensions failed. This error can happen for\nmany reasons, often due to issues with your setup or how NumPy was\ninstalled.\n\nWe have compiled some common reasons and troubleshooting tips at:\n\n    https://numpy.org/devdocs/user/troubleshooting-importerror.html\n\nPlease note and check the following:\n\n  * The Python version is: Python3.9 from \"C:\\Users\\INGRSHA\\Anaconda3\\python.exe\"\n  * The NumPy version is: \"1.21.6\"\n\nand make sure that they are the versions you expect.\nPlease carefully study the documentation linked above for further help.\n\nOriginal error was: DLL load failed while importing _multiarray_umath: The specified module could not be found.\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "_multiarray_umath failed to import",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;31mImportError\u001b[0m: _multiarray_umath failed to import"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "numpy._core.umath failed to import",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_13876\\3538788196.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpreprocessing\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mMinMaxScaler\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel_selection\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     47\u001b[0m \u001b[0m_tf2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     48\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 49\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_api\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mv2\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0m__internal__\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     50\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_api\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mv2\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0m__operators__\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     51\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_api\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mv2\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0maudio\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\_api\\v2\\__internal__\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0msys\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0m_sys\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_api\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__internal__\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mautograph\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_api\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__internal__\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mdecorator\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_api\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__internal__\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mdispatch\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\_api\\v2\\__internal__\\autograph\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0msys\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0m_sys\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautograph\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mag_ctx\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mcontrol_status_ctx\u001b[0m \u001b[1;31m# line: 34\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautograph\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimpl\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapi\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtf_convert\u001b[0m \u001b[1;31m# line: 493\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\autograph\\core\\ag_ctx.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mthreading\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 21\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautograph\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mag_logging\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     22\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mutil\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtf_export\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtf_export\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\autograph\\utils\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[1;34m\"\"\"Utility module that contains APIs usable in the generated code.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 17\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautograph\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontext_managers\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mcontrol_dependency_on_returns\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     18\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautograph\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmisc\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0malias_tensors\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautograph\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtensor_list\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mdynamic_list_append\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\autograph\\utils\\context_managers.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mcontextlib\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 19\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mframework\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mops\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     20\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mops\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtensor_array_ops\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     48\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtf2\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     49\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclient\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpywrap_tf_session\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 50\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0meager\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mcontext\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     51\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0meager\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0meager\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmonitoring\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\context.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     35\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclient\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpywrap_tf_session\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     36\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0meager\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mcancellation\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 37\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0meager\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mexecute\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     38\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0meager\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mexecutor\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     39\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0meager\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmonitoring\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpywrap_tfe\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0meager\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 21\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mframework\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mdtypes\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     22\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mframework\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtensor_conversion_registry\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mframework\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtensor_shape\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtyping\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mType\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mSequence\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mOptional\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 21\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mml_dtypes\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     22\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\ml_dtypes\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtyping\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mType\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 32\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mml_dtypes\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_finfo\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mfinfo\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     33\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mml_dtypes\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_iinfo\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0miinfo\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     34\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mml_dtypes\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_ml_dtypes_ext\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mbfloat16\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\ml_dtypes\\_finfo.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtyping\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mDict\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 19\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mml_dtypes\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_ml_dtypes_ext\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mbfloat16\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     20\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mml_dtypes\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_ml_dtypes_ext\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mfloat8_e4m3b11fnuz\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mml_dtypes\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_ml_dtypes_ext\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mfloat8_e4m3fn\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mImportError\u001b[0m: numpy._core.umath failed to import"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load the data\n",
    "train_df = pd.read_csv('train.csv')\n",
    "test_df = pd.read_csv('test.csv')\n",
    "\n",
    "# Define the features and the target variable\n",
    "features = ['Item_Weight', 'Item_Fat_Content', 'Item_Visibility', 'Item_Type', 'Item_MRP', 'Outlet_Identifier', 'Outlet_Establishment_Year', 'Outlet_Size', 'Outlet_Location_Type', 'Outlet_Type']\n",
    "target = 'Item_Outlet_Sales'\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, y_train = train_df[features], train_df[target]\n",
    "X_test, y_test = test_df[features], test_df[target]\n",
    "\n",
    "# Scale the data using MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Define the neural network model\n",
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Dense(64, activation='relu'),\n",
    "    tf.keras.layers.Dense(32, activation='relu'),\n",
    "    tf.keras.layers.Dense(16, activation='relu')\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train, epochs=100, batch_size=32)\n",
    "\n",
    "# Make predictions on the test set\n",
    "predictions = model.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "rmse = np.sqrt(np.mean((y_test - predictions) ** 2))\n",
    "print('RMSE:', rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1da387d5",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (1751429654.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"C:\\Users\\INGRSHA\\AppData\\Local\\Temp\\ipykernel_13876\\1751429654.py\"\u001b[1;36m, line \u001b[1;32m1\u001b[0m\n\u001b[1;33m    python -m pip install tensorflow\u001b[0m\n\u001b[1;37m              ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "python -m pip install tensorflow\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d19eed8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
